Used Claude Sonnet 3.7 Model for EDA Portion September 14th, 2025 @ 3:49PM 
with the following prompt:

Help me build the EDA Cheart for my Linear Regression Model assumptions 
Check Linearity with scatter plots of predictors vs. churn, component-plus-residual plots (which you've already implemented), and Ramsey's RESET test
Check Independence of Observations with Durbin-Watson test (your data doesn't appear time-series based, but worth checking)
Check Homoscedasticity with residual vs. predicted value plots, Breusch-Pagan test, and scale-location plots
Check Normality of Residuals with Q-Q plots, histograms of residuals, and Shapiro-Wilk test
Check for Multicollinearity with correlation matrix (particularly between tenure, MonthlyCharges, and TotalCharges) and Variance Inflation Factor (VIF)
Check for Influential Outliers with Cook's distance and leverage plots (especially for high monthly/total charges)

<MY CODE SO FAR>
# AIPI 590 - XAI | Assignment 2 - Interpretable ML
### Description
### Christian Moreira

#### Include the button below. Change the link to the location in your github repository:
#### Example: https://colab.research.google.com/github/yourGHName/yourREPOName/blob/yourBranchName/yourFileName.ipynb


[![Open In Collab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/U1186204/XAI-Interpretable-Machine-Learning/blob/main/Interpretable_ML_Chris.ipynb)
# Install Packages
# Load Packages
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.metrics import mean_squared_error, r2_score
import statsmodels.api as sm
import statsmodels.formula.api as smf
from statsmodels.graphics.regressionplots import plot_ccpr
from statsmodels.stats.diagnostic import linear_reset
import kagglehub
from kagglehub import KaggleDatasetAdapter
# Data Load
**The Churn dataset used in this assignment is derived from Kaggle and can be found [HERE](https://www.kaggle.com/datasets/blastchar/telco-customer-churn/data)**
# Set the path to the main CSV file (hardcoded, since listing isn't supported)
file_path = "WA_Fn-UseC_-Telco-Customer-Churn.csv"

df = kagglehub.load_dataset(
    KaggleDatasetAdapter.PANDAS,
    "blastchar/telco-customer-churn",
    file_path
)
# Task 1
Exploratory Data Analysis to check Assumptions: Perform an exploratory analysis of the dataset to understand the relationships between different features and the target variable (churn). Use appropriate visualizations and statistical methods to determine whether assumptions about linear, logistic, and GAM models are met. 
### Data Dimensions, Variable Types, and Check for Missing Values
print("Data Structure")
print("-----------------")
print(f"Dimensions: {df.shape}")
print(f"Data Types:\n{df.dtypes}")
print(f"Missing Values:\n {df.isnull().sum()}")
### Interpretation
- Dataset has a total of 7043 observations & 21 variables
- With the exception of MonthlyCharges, tenure and senior citizen - most other variables seem categorical, including the explained variable **Churn**
- No missing fields have been reported suggesting there is not a need for imputations or deletion of observations
### Central tendencies for numerical variables, measures of dispersion for numerical variables, distributions
print("\n Descriptive Statistics for Numerical Variables")
print("------------------")
numeric_columns = df.select_dtypes(include=np.number).columns
print("Central Tendency Measures:")
print(df[numeric_columns].describe().loc[['mean', '50%']])
print("\n Dispersion Measures:")
print(df[numeric_columns].describe().loc[['std', 'min', 'max']])

# Distribution Normality Check
print("\n Distribution Measures:")
print(df[numeric_columns].skew())
print(df[numeric_columns].kurt())
### Interpretation
**Descriptive Statistics**
- The variable SeniorCitizen appears to be a binary variable indicating whether an individual is a senior or not. 
- Means and medians provide insights into the central location of the data. The mean tenure is ~32 months and the median 29 months. The mean monthly charges are ~$64 and median ~$70. 

**Dispersion Measures**
- The skewness value for tenure of ~.23, which is positive and close to 0 indicates a slight right skew. Similarly, the skewness value of ~-.22 for monthly charges, while close to 0 and negative, suggests a slight left skew. 
- The  kurtosis value for tenure and monthly charges are -1.38 and -1.25 respectively; These indicate values are spread out and with thin tails, which mean distributions for tenure months and monthly charges in $ should be relatively uniform.  
### Checking for Overall Data Quality: Duplicated values & Outliers 
# Data Quanlity Checks
print("\n Data Quality")
print("------------------")
print(f"Duplicated Rows: {df.duplicated().sum()}")
print("Checking for Inconsistent Values:")
print(df.apply(lambda x: x.value_counts().index[0]).to_frame(name='Most Frequent Value'))
### Interpretation
**Data Quality Assessment**
- The dataset does not exhibit any inconsistencies: there are no duplicated values or nonsensical values in the dataset. 
### Data Encoding for Model EDA
- Given a moajority of the dataset is Categorical we will do come encodings in order to visualize the relationship between explanatory variables and explained variable(Churn)
# Only Tenure and MonthlyCharges are numeric
numeric_columns = ['tenure', 'MonthlyCharges', 'TotalCharges']
df['TotalCharges'] = df['TotalCharges'].replace(' ', 0)
df[numeric_columns] = df[numeric_columns].astype(float)

# Cleaning Internet and Phone Services and Encoding them to Binary
internet_dependent_services = ['OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 
                              'TechSupport', 'StreamingTV', 'StreamingMovies']
for col in internet_dependent_services:
    df[col] = df[col].replace({'No internet service': 'No'})
    df[col] = df[col].map({'Yes': 1, 'No': 0})

df['MultipleLines'] = df['MultipleLines'].replace({'No phone service': 'No'})
df['MultipleLines'] = df['MultipleLines'].map({'Yes': 1, 'No': 0})

# Encoding all other Binary Variables to 0 and 1, Including Churn
df['gender'] = df['gender'].map({'Female': 0, 'Male': 1})
df['Partner'] = df['Partner'].map({'No': 0, 'Yes': 1})
df['Dependents'] = df['Dependents'].map({'No': 0, 'Yes': 1})
df['PhoneService'] = df['PhoneService'].map({'No': 0, 'Yes': 1})
df['PaperlessBilling'] = df['PaperlessBilling'].map({'No': 0, 'Yes': 1})
df['Churn'] = df['Churn'].map({'No': 0, 'Yes': 1})

# Encoding Categorical Variables with more than 2 categories using One-Hot Encoding
df['InternetService'] = df['InternetService'].map({'DSL': 1, 'Fiber optic': 2, 'No': 0})
df['Contract'] = df['Contract'].map({'Month-to-month': 0, 'One year': 1, 'Two year': 2})
df['PaymentMethod'] = df['PaymentMethod'].map({'Electronic check': 0, 'Mailed check': 1, 
                                               'Bank transfer (automatic)': 2, 'Credit card (automatic)': 3})

# Print Only the NUMBER of unique values in each column
print("\n Unique Values per Column")
print("------------------")
print(df.nunique())
# Only Tenure and MonthlyCharges are numeric
numeric_columns = ['tenure', 'MonthlyCharges', 'TotalCharges']
df['TotalCharges'] = df['TotalCharges'].replace(' ', 0)
df[numeric_columns] = df[numeric_columns].astype(float)

# Cleaning Internet and Phone Services and Encoding them to Binary
internet_dependent_services = ['OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 
                              'TechSupport', 'StreamingTV', 'StreamingMovies']
for col in internet_dependent_services:
    df[col] = df[col].replace({'No internet service': 'No'})
    df[col] = df[col].map({'Yes': 1, 'No': 0})

df['MultipleLines'] = df['MultipleLines'].replace({'No phone service': 'No'})
df['MultipleLines'] = df['MultipleLines'].map({'Yes': 1, 'No': 0})

# Encoding all other Binary Variables to 0 and 1, Including Churn
df['gender'] = df['gender'].map({'Female': 0, 'Male': 1})
df['Partner'] = df['Partner'].map({'No': 0, 'Yes': 1})
df['Dependents'] = df['Dependents'].map({'No': 0, 'Yes': 1})
df['PhoneService'] = df['PhoneService'].map({'No': 0, 'Yes': 1})
df['PaperlessBilling'] = df['PaperlessBilling'].map({'No': 0, 'Yes': 1})
df['Churn'] = df['Churn'].map({'No': 0, 'Yes': 1})

# Encoding Categorical Variables with more than 2 categories using One-Hot Encoding
df['InternetService'] = df['InternetService'].map({'DSL': 1, 'Fiber optic': 2, 'No': 0})
df['Contract'] = df['Contract'].map({'Month-to-month': 0, 'One year': 1, 'Two year': 2})
df['PaymentMethod'] = df['PaymentMethod'].map({'Electronic check': 0, 'Mailed check': 1, 
                                               'Bank transfer (automatic)': 2, 'Credit card (automatic)': 3})

# Print Only the NUMBER of unique values in each column
print("\n Unique Values per Column")
print("------------------")
print(df.nunique())
### Linearity Check

SUMMARY OF DATA>

 Unique Values per Column
------------------
customerID          7043
gender                 2
SeniorCitizen          2
Partner                2
Dependents             2
tenure                73
PhoneService           2
MultipleLines          2
InternetService        3
OnlineSecurity         2
OnlineBackup           2
DeviceProtection       2
TechSupport            2
StreamingTV            2
StreamingMovies        2
Contract               3
PaperlessBilling       2
PaymentMethod          4
MonthlyCharges      1585
TotalCharges        6531
Churn                  2
dtype: int64
__________________________________________________________________________________________________________________________________________________________________________________

Used Claude Sonnet 3.7 Model for EDA Portion September 14th, 2025 @ 6:44PM 
with the following prompt

Help me build the EDA Cheart Now for my Logisitc and GAM Models assumptions. There is one Caveat, we only build the chart if it is different from eda we did for the linear model

EDA for linear model
Check Linearity with scatter plots of predictors vs. churn, component-plus-residual plots (which you've already implemented), and Ramsey's RESET test
Check Independence of Observations with Durbin-Watson test (your data doesn't appear time-series based, but worth checking)
Check Homoscedasticity with residual vs. predicted value plots, Breusch-Pagan test, and scale-location plots
Check Normality of Residuals with Q-Q plots, histograms of residuals, and Shapiro-Wilk test
Check for Multicollinearity with correlation matrix (particularly between tenure, MonthlyCharges, and TotalCharges) and Variance Inflation Factor (VIF)
Check for Influential Outliers with Cook's distance and leverage plots (especially for high monthly/total charges)

EDA for Logictic
Check Linearity of the logit with Box-Tidwell test for continuous predictors (tenure, MonthlyCharges, TotalCharges)
Check Independence of Observations by examining data collection methodology (telecom customer data typically meets this)
Check for Multicollinearity with correlation matrix and VIF (especially important for your encoded categorical variables)
Check for Complete or Quasi-Complete Separation by examining convergence warnings and coefficient standard errors
Check for Influential Outliers with Cook's distance and standardized residual plots
Check for Adequate Sample Size by ensuring 10+ events per predictor (your churn=1 cases relative to predictors)
Check Calibration with calibration plots to ensure predicted probabilities match observed frequencies

EDA for GAM
Check for Non-linear Relationships with partial residual plots for continuous variables (tenure, MonthlyCharges, TotalCharges)
Check Independence of Observations same as for logistic regression
Check for Appropriate Smoothing Parameters with GCV (Generalized Cross-Validation) score
Check Distribution Assumption with Q-Q plots of deviance residuals
Check for Influential Observations with Cook's distance and leverage plots
Check for Concurvity (GAM's version of multicollinearity) with concurvity tests
Check Model Complexity vs. Performance with AIC/BIC metrics and k-fold cross-validation

Provide code in python and provide code chunks split for each model-assumption you are validating. Remember, no need to double code if the model-asusmtion is the same than linear assumptions already validated.  
__________________________________________________________________________________________________________________________________________________________________________________
Used Claude Sonnet 3.7 Model for Chart Creation Logistic Regression Cells ON September 15th, 2025 @ 1:54 AM 
with the following prompt

Help me create the necessary charts for evaluating my Logistic Model, note my code so far for Linear model
X = df.drop(['customerID', 'Churn'], axis=1)
y = df['Churn']

# X['log_tenure'] = np.log1p(X['tenure'])
X['log_TotalCharges'] = np.log1p(X['TotalCharges'])
X['log_MonthlyCharges'] = np.log1p(X['MonthlyCharges'])

# Removing Internet Services & Low explainability variables
X = X.drop(['InternetService', 'tenure', 'MonthlyCharges', 'TotalCharges', 'gender', 'OnlineBackup', 'Partner', 'DeviceProtection', 'StreamingTV'], axis=1)
# X = X.drop(['tenure', 'MonthlyCharges', 'TotalCharges'], axis=1)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


# In[146]:


def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    print(f"\n{model_name}:")
    print(f"MSE: {mse:.4f}")
    print(f"R2 Score: {r2:.4f}")

    return model, y_pred


# In[147]:


# Fit models
lr_model, lr_pred = evaluate_model(LinearRegression(), X_train_scaled, X_test_scaled, y_train, y_test, "Linear Regression")


# In[148]:


model = sm.OLS(y, X).fit()
print(model.summary())


# In[166]:


def plot_regression_diagnostics(y_test, y_pred, model_name):
    residuals = y_test - y_pred

    # Create figure with 1 row and 2 columns
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))

    # Residual Plot (left subplot)
    ax1.scatter(y_pred, residuals, alpha=0.5)
    ax1.set_xlabel('Predicted Values')
    ax1.set_ylabel('Residuals')
    ax1.set_title(f'Residual Plot - {model_name}')
    ax1.axhline(y=0, color='r', linestyle='--')

    # Add a LOWESS smoother to check for patterns
    lowess_result = lowess(residuals, y_pred, frac=0.6)
    ax1.plot(lowess_result[:, 0], lowess_result[:, 1], 'g-', linewidth=2, label='LOWESS Trend')
    ax1.legend()
    ax1.grid(True, alpha=0.3)

    # Q-Q Plot (right subplot)
    stats.probplot(residuals, plot=ax2)
    ax2.set_title(f'Q-Q Plot - {model_name}')
    ax2.grid(True, alpha=0.3)

    # Add statistical test results as text
    shapiro_test = stats.shapiro(residuals)
    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)
    textstr = '\n'.join((
        'Shapiro-Wilk Test:',
        f'W={shapiro_test[0]:.4f}',
        f'p-value={shapiro_test[1]:.4f}'
    ))
    ax2.text(0.05, 0.95, textstr, transform=ax2.transAxes, fontsize=10,
             verticalalignment='top', bbox=props)

    plt.tight_layout()
    plt.suptitle(f'Regression Diagnostics: {model_name}', y=1.05, fontsize=16)
    plt.show()

    # Additional statistics
    print(f"\nDiagnostic Statistics - {model_name}:")
    print(f"Mean of Residuals: {np.mean(residuals):.4f}")
    print(f"Standard Deviation of Residuals: {np.std(residuals):.4f}")

    # Heteroscedasticity test
    try:
        X_bp = sm.add_constant(y_pred)
        bp_test = het_breuschpagan(residuals, X_bp)
        print(f"\nBreusch-Pagan Test for Heteroscedasticity:")
        print(f"LM Statistic: {bp_test[0]:.4f}")
        print(f"p-value: {bp_test[1]:.4f}")

    except Exception as e:
        print(f"Could not perform heteroscedasticity test: {str(e)}")

# Example usage - only for Linear Regression
plot_regression_diagnostics(y_test, lr_pred, 'Linear Regression')


# In[164]:


def plot_model_evaluation(y_true, y_pred, model_name):
    # Create a figure with 1 row and 2 columns
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

    # Confusion Matrix (left subplot)
    cm = confusion_matrix(y_true, np.round(y_pred))
    sns.heatmap(
        cm,
        annot=True,
        fmt="d",
        cmap="Blues",
        xticklabels=["No Churn", "Churn"],
        yticklabels=["No Churn", "Churn"],
        ax=ax1
    )
    ax1.set_xlabel("Predicted")
    ax1.set_ylabel("Actual")
    ax1.set_title(f"Confusion Matrix - {model_name}")

    # ROC Curve (right subplot)
    fpr, tpr, _ = roc_curve(y_true, y_pred)
    roc_auc = roc_auc_score(y_true, y_pred)
    ax2.plot(fpr, tpr, label=f"ROC curve (area = {roc_auc:.2f})")
    ax2.plot([0, 1], [0, 1], "k--")
    ax2.set_xlabel("False Positive Rate")
    ax2.set_ylabel("True Positive Rate")
    ax2.set_title(f"ROC Curve - {model_name}")
    ax2.legend(loc="lower right")

    plt.tight_layout()
    plt.suptitle(f"Model Evaluation: {model_name}", y=1.05, fontsize=16)
    plt.show()


    return roc_auc

# RPint ROC and COnfusion Matrix for Linear Regression
roc_auc_lr = plot_model_evaluation(y_test, lr_pred, "Linear Regression")


# ### Coefficient Interpretation Linear Regression
# 
# | Variable | Interpretation |
# |----------|----------------|
# | SeniorCitizen | Being a senior citizen increases the probability of churning by about 6.1%. This is statistically significant (p < 0.001), which means senior citizens are more likely to leave compared to non-seniors. |
# | Dependents | Having dependents slightly decreases the likelihood of churning by 1.7%. However, this isn't statistically significant (p = 0.09), so we can't be confident that dependents actually impact churn rates. |
# | PhoneService | Having phone service decreases churn probability by about 10.9%. This is highly significant (p < 0.001), suggesting customers with phone service are more loyal to the company. |
# | MultipleLines | Customers with multiple lines are 5.6% more likely to churn. This effect is statistically significant (p < 0.001), which might indicate that customers with more complex services find more reasons to leave. |
# | OnlineSecurity | Having online security decreases churn by 9.6%. This significant effect (p < 0.001) suggests that customers who feel their data is secure are more likely to stay with the company. |
# | TechSupport | Tech support reduces churn probability by 9.5%. This significant effect (p < 0.001) indicates that customers who receive technical assistance are more satisfied and less likely to leave. |
# | StreamingMovies | Customers with streaming movies are 2.5% more likely to churn. This is statistically significant (p = 0.019), though the effect is smaller than other services. |
# | Contract | Each step up in contract length (from month-to-month to one year to two years) decreases churn by 5.2%. This significant effect (p < 0.001) shows longer contracts help retain customers. |
# | PaperlessBilling | Customers with paperless billing are 5.6% more likely to churn. This significant effect (p < 0.001) might suggest these customers are more tech-savvy and willing to switch providers. |
# | PaymentMethod | Each step up in payment method reduces churn probability by 2.5%. This significant effect (p < 0.001) suggests certain payment methods are associated with higher customer loyalty. |
# | log_TotalCharges | A 1% increase in total charges is associated with a 0.11% decrease in churn probability. This highly significant effect (p < 0.001) suggests customers who have spent more with the company over time are less likely to leave. |
# | log_MonthlyCharges | A 1% increase in monthly charges is associated with a 0.29% increase in churn probability. This highly significant effect (p < 0.001) shows that customers with higher monthly bills are much more likely to churn. |

# ### Linear Regression Model Results Interpretation
# | Model Element | Interpretation |
# |----------|----------------|
# | ROC Score | A value of .83 indicates the model has a solid in predicting Churn |
# | R^2 Test | A value of .2567 suggests the model performs just moderately on unseen data |
# | R^2  | A value of .475 suggests the explains 47.5% of the variability in Churn |
# | Auto-Correlation  | After careful variable selection the model did not identify auto correlation with a Durbin Watson Score of 2.01 |
# | Heteroscedasticity | The Breusch-Pagan Test for Heteroscedasticity(LM Statistic: 199.0076 p-value: 0.0000) suggests theere is Heteroscedasticity in residuals |
# | Normality | The Shapiro-Wilk test result (W = 0.9588) indicates there could be a mild deviation from normality in the model residuals. This deviation is observed in the QQ plot where the blue dots slightly deviate from the red line at the tails indicating a relative skew on the data |
# 


This time the only difference in that I need charts that are relevant for logistic only. Keep COnfusion matrix and ROC Curve please
__________________________________________________________________________________________________________________________________________________________________________________
Used Claude Sonnet 3.7 Model for GAM Model visualizations on September 14th, 2025 @ 2:16 AM 
with the following prompt



__________________________________________________________________________________________________________________________________________________________________________________
Used Claude Sonnet 3.7 Model for Cell X on Notebook Cell [] September 1th, 2025 @ 3:49PM 
with the following prompt

__________________________________________________________________________________________________________________________________________________________________________________
Used Claude Sonnet 3.7 Model for Cell X on Notebook Cell [] September 1th, 2025 @ 3:49PM 
with the following prompt

__________________________________________________________________________________________________________________________________________________________________________________
Used Claude Sonnet 3.7 Model for Cell X on Notebook Cell [] September 1th, 2025 @ 3:49PM 
with the following prompt

__________________________________________________________________________________________________________________________________________________________________________________
Used Claude Sonnet 3.7 Model for Cell X on Notebook Cell [] September 1th, 2025 @ 3:49PM 
with the following prompt

__________________________________________________________________________________________________________________________________________________________________________________
Used Claude Sonnet 3.7 Model for Cell X on Notebook Cell [] September 1th, 2025 @ 3:49PM 
with the following prompt

__________________________________________________________________________________________________________________________________________________________________________________
Used Claude Sonnet 3.7 Model for Cell X on Notebook Cell [] September 1th, 2025 @ 3:49PM 
with the following prompt

__________________________________________________________________________________________________________________________________________________________________________________
Used Claude Sonnet 3.7 Model for Cell X on Notebook Cell [] September 1th, 2025 @ 3:49PM 
with the following prompt

__________________________________________________________________________________________________________________________________________________________________________________
Used Claude Sonnet 3.7 Model for Cell X on Notebook Cell [] September 1th, 2025 @ 3:49PM 
with the following prompt


__________________________________________________________________________________________________________________________________________________________________________________
Used Claude Sonnet 3.7 Model for Cell X on Notebook Cell [] September 1th, 2025 @ 3:49PM 
with the following prompt
Used Claude Sonnet 3.7 Model for Cell X on Notebook Cell [] September 1th, 2025 @ 3:49PM 
with the following prompt
Used Claude Sonnet 3.7 Model for Cell X on Notebook Cell [] September 1th, 2025 @ 3:49PM 
with the following prompt
Used Claude Sonnet 3.7 Model for Cell X on Notebook Cell [] September 1th, 2025 @ 3:49PM 
with the following prompt
Used Claude Sonnet 3.7 Model for Cell X on Notebook Cell [] September 1th, 2025 @ 3:49PM 
with the following prompt
Used Claude Sonnet 3.7 Model for Cell X on Notebook Cell [] September 1th, 2025 @ 3:49PM 
with the following prompt
Used Claude Sonnet 3.7 Model for Cell X on Notebook Cell [] September 1th, 2025 @ 3:49PM 
with the following prompt
Used Claude Sonnet 3.7 Model for Cell X on Notebook Cell [] September 1th, 2025 @ 3:49PM 
with the following prompt